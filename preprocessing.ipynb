{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c25d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59e9055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our training data and take the first column as the index column\n",
    "fraud_train = pd.read_csv('fraudTrain.csv', index_col=0)\n",
    "\n",
    "# split the training data into x and y\n",
    "X = fraud_train.drop(columns='is_fraud')\n",
    "y = fraud_train['is_fraud']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "652b9429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   trans_date_trans_time  1296675 non-null  object \n",
      " 1   cc_num                 1296675 non-null  int64  \n",
      " 2   merchant               1296675 non-null  object \n",
      " 3   category               1296675 non-null  object \n",
      " 4   amt                    1296675 non-null  float64\n",
      " 5   first                  1296675 non-null  object \n",
      " 6   last                   1296675 non-null  object \n",
      " 7   gender                 1296675 non-null  object \n",
      " 8   street                 1296675 non-null  object \n",
      " 9   city                   1296675 non-null  object \n",
      " 10  state                  1296675 non-null  object \n",
      " 11  zip                    1296675 non-null  int64  \n",
      " 12  lat                    1296675 non-null  float64\n",
      " 13  long                   1296675 non-null  float64\n",
      " 14  city_pop               1296675 non-null  int64  \n",
      " 15  job                    1296675 non-null  object \n",
      " 16  dob                    1296675 non-null  object \n",
      " 17  trans_num              1296675 non-null  object \n",
      " 18  unix_time              1296675 non-null  int64  \n",
      " 19  merch_lat              1296675 non-null  float64\n",
      " 20  merch_long             1296675 non-null  float64\n",
      "dtypes: float64(5), int64(4), object(12)\n",
      "memory usage: 217.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94102</th>\n",
       "      <td>2019-02-25 08:24:40</td>\n",
       "      <td>374497717543058</td>\n",
       "      <td>fraud_Funk Group</td>\n",
       "      <td>grocery_net</td>\n",
       "      <td>20.00</td>\n",
       "      <td>Linda</td>\n",
       "      <td>Hurst</td>\n",
       "      <td>F</td>\n",
       "      <td>31701 Tucker Square Suite 893</td>\n",
       "      <td>Wilton</td>\n",
       "      <td>...</td>\n",
       "      <td>58579</td>\n",
       "      <td>47.1709</td>\n",
       "      <td>-100.7944</td>\n",
       "      <td>1190</td>\n",
       "      <td>Designer, ceramics/pottery</td>\n",
       "      <td>1948-06-30</td>\n",
       "      <td>1595dec12f6f19ceaae9b7df0f8af5c0</td>\n",
       "      <td>1330158280</td>\n",
       "      <td>46.398331</td>\n",
       "      <td>-99.813959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198791</th>\n",
       "      <td>2019-04-12 19:50:15</td>\n",
       "      <td>4428154703770710</td>\n",
       "      <td>fraud_Prosacco, Kreiger and Kovacek</td>\n",
       "      <td>home</td>\n",
       "      <td>284.88</td>\n",
       "      <td>Brittany</td>\n",
       "      <td>Guerra</td>\n",
       "      <td>F</td>\n",
       "      <td>79209 Gary Dale</td>\n",
       "      <td>Colton</td>\n",
       "      <td>...</td>\n",
       "      <td>99113</td>\n",
       "      <td>46.5901</td>\n",
       "      <td>-117.1692</td>\n",
       "      <td>761</td>\n",
       "      <td>Chief Marketing Officer</td>\n",
       "      <td>1943-06-30</td>\n",
       "      <td>0ed26b649ed0fce94d8e632b7208dea0</td>\n",
       "      <td>1334260215</td>\n",
       "      <td>45.687331</td>\n",
       "      <td>-117.488135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238587</th>\n",
       "      <td>2020-05-31 21:50:53</td>\n",
       "      <td>213148039875802</td>\n",
       "      <td>fraud_Langworth, Boehm and Gulgowski</td>\n",
       "      <td>shopping_net</td>\n",
       "      <td>5.07</td>\n",
       "      <td>Jill</td>\n",
       "      <td>Jacobs</td>\n",
       "      <td>F</td>\n",
       "      <td>034 Kimberly Mountains</td>\n",
       "      <td>Brandon</td>\n",
       "      <td>...</td>\n",
       "      <td>33510</td>\n",
       "      <td>27.9551</td>\n",
       "      <td>-82.2966</td>\n",
       "      <td>79613</td>\n",
       "      <td>Environmental consultant</td>\n",
       "      <td>1978-11-30</td>\n",
       "      <td>7096316ec1a4b261e8613013827abae7</td>\n",
       "      <td>1370037053</td>\n",
       "      <td>27.254081</td>\n",
       "      <td>-81.974799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        trans_date_trans_time            cc_num  \\\n",
       "94102     2019-02-25 08:24:40   374497717543058   \n",
       "198791    2019-04-12 19:50:15  4428154703770710   \n",
       "1238587   2020-05-31 21:50:53   213148039875802   \n",
       "\n",
       "                                     merchant      category     amt     first  \\\n",
       "94102                        fraud_Funk Group   grocery_net   20.00     Linda   \n",
       "198791    fraud_Prosacco, Kreiger and Kovacek          home  284.88  Brittany   \n",
       "1238587  fraud_Langworth, Boehm and Gulgowski  shopping_net    5.07      Jill   \n",
       "\n",
       "           last gender                         street     city  ...    zip  \\\n",
       "94102     Hurst      F  31701 Tucker Square Suite 893   Wilton  ...  58579   \n",
       "198791   Guerra      F                79209 Gary Dale   Colton  ...  99113   \n",
       "1238587  Jacobs      F         034 Kimberly Mountains  Brandon  ...  33510   \n",
       "\n",
       "             lat      long  city_pop                         job         dob  \\\n",
       "94102    47.1709 -100.7944      1190  Designer, ceramics/pottery  1948-06-30   \n",
       "198791   46.5901 -117.1692       761     Chief Marketing Officer  1943-06-30   \n",
       "1238587  27.9551  -82.2966     79613    Environmental consultant  1978-11-30   \n",
       "\n",
       "                                trans_num   unix_time  merch_lat  merch_long  \n",
       "94102    1595dec12f6f19ceaae9b7df0f8af5c0  1330158280  46.398331  -99.813959  \n",
       "198791   0ed26b649ed0fce94d8e632b7208dea0  1334260215  45.687331 -117.488135  \n",
       "1238587  7096316ec1a4b261e8613013827abae7  1370037053  27.254081  -81.974799  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first look at our data\n",
    "X.info()\n",
    "X.sample(3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f775f22",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## 1. Finding and Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b01dbe95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what columns have null values?\n",
    "sum(X.isnull().sum()) + y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5586ff34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what columns have missing/na values?\n",
    "sum(X.isna().sum()) + y.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42263e5e",
   "metadata": {},
   "source": [
    "No handling of missing values is required as there are no missing values in our training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33756333",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Finding and Removing Outliers\n",
    "\n",
    "Removing outliers is an important part of preprocessing as outliers can:\n",
    "- distort data analysis \n",
    "- reduce machine learning model accuracy and generalization\n",
    "- impact visual data, skewing the scale \n",
    "- more?\n",
    "\n",
    "For numeric columns (those with an integer or float data type), I chose to use the [Interquartile Range Method](https://online.stat.psu.edu/stat200/lesson/3/3.2) of finding and removing outliers. \n",
    "\n",
    "**Categorical, or ...**\n",
    "\n",
    "# FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16038acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the numerical columns\n",
    "num_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "num_cols\n",
    "\n",
    "# calculate the IQR lower and upper bounds for each numerical column\n",
    "def iqr_bounds(col):\n",
    "    Q1 = col.quantile(0.25)\n",
    "    Q3 = col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb56025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_num: lower=-6513275846701038.0, upper=1.133557426847813e+16\n",
      "amt: lower=-100.58499999999998, upper=193.375\n",
      "zip: lower=-42470.5, upper=140749.5\n",
      "lat: lower=23.640650000000004, upper=52.920249999999996\n",
      "long: lower=-121.75800000000001, upper=-55.198\n",
      "city_pop: lower=-28634.5, upper=49705.5\n",
      "unix_time: lower=1307798793.0, upper=1390337325.0\n",
      "merch_lat: lower=23.898184000000008, upper=52.79255199999999\n",
      "merch_long: lower=-121.88799400000002, upper=-55.24607799999998\n"
     ]
    }
   ],
   "source": [
    "# lets look at the lower and upper bounds for each numerical column\n",
    "for col in num_cols:\n",
    "    lower, upper = iqr_bounds(X[col])\n",
    "    print(f\"{col}: lower={lower}, upper={upper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a261fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_num: 118789 outliers\n",
      "amt: 67290 outliers\n",
      "zip: 0 outliers\n",
      "lat: 4679 outliers\n",
      "long: 49922 outliers\n",
      "city_pop: 242674 outliers\n",
      "unix_time: 0 outliers\n",
      "merch_lat: 4967 outliers\n",
      "merch_long: 41994 outliers\n"
     ]
    }
   ],
   "source": [
    "# how many outliers are there in each numerical column?\n",
    "for col in num_cols:\n",
    "    lower, upper = iqr_bounds(X[col])\n",
    "    outliers = fraud_train[(fraud_train[col] < lower) | (fraud_train[col] > upper)]\n",
    "    print(f\"{col}: {len(outliers)} outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddcae0",
   "metadata": {},
   "source": [
    "Some of the numerical columns wouldn't make sense to have outlier values, such as cc_num. However, I won't be dropping any of the rows with outlier values before training a model on the dataset, as I'm interested in comparing how the model performs with and without the outlier values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6447216",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Feature Selection & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d0dbf",
   "metadata": {},
   "source": [
    "Before training a model, I want to get a good idea of our data. I ask myself questions such as *\"is this information redundnat?\"* or *\"can this data be generalized or are all values unique?\"*. If the answer to either question is yes, it is usually best to remove the column. However, sometimes there is information that we can extract from the noisy column that may not be redundant and can be generalized or categorized. \n",
    "\n",
    "\n",
    "One example in our data is the trans_time_trans_date column. At the moment, this column may not help us detect fraud very well since almost every entry is unqiue (as shown below), but there is some interesting information we may want to look at within the column. Information such as day of the week or the hour in which the transaction took place could help us identify fraudulent transactions more accurately. Based on our columns printed below, I plan to extract the following information:\n",
    "\n",
    "- cc_num: the number of transaction in a given day for each credit card number\n",
    "- trans_date_trans_time: the month, day of week, and hour of transaction\n",
    "- dob: the year of birth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "755eb76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.31% of the values in trans_date_trans_time are unique.\n",
      "0.08% of the values in cc_num are unique.\n",
      "0.05% of the values in merchant are unique.\n",
      "0.0% of the values in category are unique.\n",
      "4.08% of the values in amt are unique.\n",
      "0.03% of the values in first are unique.\n",
      "0.04% of the values in last are unique.\n",
      "0.0% of the values in gender are unique.\n",
      "0.08% of the values in street are unique.\n",
      "0.07% of the values in city are unique.\n",
      "0.0% of the values in state are unique.\n",
      "0.07% of the values in zip are unique.\n",
      "0.07% of the values in lat are unique.\n",
      "0.07% of the values in long are unique.\n",
      "0.07% of the values in city_pop are unique.\n",
      "0.04% of the values in job are unique.\n",
      "0.07% of the values in dob are unique.\n",
      "100.0% of the values in trans_num are unique.\n",
      "98.31% of the values in unix_time are unique.\n",
      "96.23% of the values in merch_lat are unique.\n",
      "98.39% of the values in merch_long are unique.\n"
     ]
    }
   ],
   "source": [
    "def unique_percentage(column):\n",
    "    return (len(X[column].unique())) / len(X[column]) * 100\n",
    "\n",
    "for column in X.columns:\n",
    "    print(f'{round(unique_percentage(column),2)}% of the values in {column} are unique.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de5f3d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming 'trans_date_trans_time' into day of week, hour of day, and month\n",
    "X['trans_date_trans_time'] = pd.to_datetime(X['trans_date_trans_time'])\n",
    "X['trans_month'] = X['trans_date_trans_time'].dt.month_name()\n",
    "X['trans_day_of_week'] = X['trans_date_trans_time'].dt.day_name()\n",
    "X['trans_hour_of_day'] = X['trans_date_trans_time'].dt.hour\n",
    "\n",
    "\n",
    "# X[['trans_month', 'trans_day_of_week', 'trans_hour_of_day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fec3d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>trans_month</th>\n",
       "      <th>trans_day_of_week</th>\n",
       "      <th>trans_hour_of_day</th>\n",
       "      <th>trans_date</th>\n",
       "      <th>trans_count_per_day</th>\n",
       "      <th>birth_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>...</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 00:00:44</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>...</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>...</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  trans_date_trans_time            cc_num                         merchant  \\\n",
       "0   2019-01-01 00:00:18  2703186189652095       fraud_Rippin, Kub and Mann   \n",
       "1   2019-01-01 00:00:44      630423337322  fraud_Heller, Gutmann and Zieme   \n",
       "2   2019-01-01 00:00:51    38859492057661             fraud_Lind-Buckridge   \n",
       "\n",
       "        category     amt      first     last gender  \\\n",
       "0       misc_net    4.97   Jennifer    Banks      F   \n",
       "1    grocery_pos  107.23  Stephanie     Gill      F   \n",
       "2  entertainment  220.11     Edward  Sanchez      M   \n",
       "\n",
       "                         street            city  ...  \\\n",
       "0                561 Perry Cove  Moravian Falls  ...   \n",
       "1  43039 Riley Greens Suite 393          Orient  ...   \n",
       "2      594 White Dale Suite 530      Malad City  ...   \n",
       "\n",
       "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
       "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
       "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
       "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
       "\n",
       "   trans_month trans_day_of_week trans_hour_of_day  trans_date  \\\n",
       "0      January           Tuesday                 0  2019-01-01   \n",
       "1      January           Tuesday                 0  2019-01-01   \n",
       "2      January           Tuesday                 0  2019-01-01   \n",
       "\n",
       "   trans_count_per_day  birth_year  \n",
       "0                    3        1988  \n",
       "1                   10        1978  \n",
       "2                    1        1962  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull month, day, year from trans_date_trans_time into one value\n",
    "X['trans_date'] = pd.to_datetime(X['trans_date_trans_time']).dt.date\n",
    "\n",
    "# count the number of transactions per credit card number per day and add it as a new column\n",
    "X['trans_count_per_day'] = X.groupby(['cc_num', 'trans_date'])['trans_date_trans_time'].transform('count')\n",
    "\n",
    "# extracting the year from the dob column\n",
    "X['birth_year'] = pd.to_datetime(X['dob']).dt.year\n",
    "\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476d149",
   "metadata": {},
   "source": [
    "Now that I've extracted all the data I needed, I want to take a closer look at the merchant column. It appears that the text \"fraud_\" preceeds every value regardless of if it's a fraudulent transaction or not. I want to remove this, as we will not see \"fraud_\" preceed merchants in a real world example. Furtehrmore, leaving it in could skew our future model in thinking if this text is seen in the merchant, then there's a higher likelihood that the transaction is fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa53889c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94102</th>\n",
       "      <td>fraud_Funk Group</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198791</th>\n",
       "      <td>fraud_Prosacco, Kreiger and Kovacek</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238587</th>\n",
       "      <td>fraud_Langworth, Boehm and Gulgowski</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619078</th>\n",
       "      <td>fraud_Conroy-Emard</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573850</th>\n",
       "      <td>fraud_Adams-Barrows</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     merchant  is_fraud\n",
       "94102                        fraud_Funk Group         0\n",
       "198791    fraud_Prosacco, Kreiger and Kovacek         0\n",
       "1238587  fraud_Langworth, Boehm and Gulgowski         0\n",
       "619078                     fraud_Conroy-Emard         0\n",
       "573850                    fraud_Adams-Barrows         0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_train[['merchant', 'is_fraud']].sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "996068ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% of merchant values contain \"fraud_\" in the name.\n"
     ]
    }
   ],
   "source": [
    "# % of merchant values that have fraud_ in the name\n",
    "print(f\"{X['merchant'].str.contains('fraud_').sum() / len(X['merchant']) * 100}% of merchant values contain \\\"fraud_\\\" in the name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d6f8da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94102</th>\n",
       "      <td>Funk Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198791</th>\n",
       "      <td>Prosacco, Kreiger and Kovacek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238587</th>\n",
       "      <td>Langworth, Boehm and Gulgowski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619078</th>\n",
       "      <td>Conroy-Emard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573850</th>\n",
       "      <td>Adams-Barrows</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               merchant\n",
       "94102                        Funk Group\n",
       "198791    Prosacco, Kreiger and Kovacek\n",
       "1238587  Langworth, Boehm and Gulgowski\n",
       "619078                     Conroy-Emard\n",
       "573850                    Adams-Barrows"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing 'fraud_' from the merchant column\n",
    "X['merchant'] = X['merchant'].str.replace('fraud_', '')\n",
    "\n",
    "X[['merchant']].sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49f2ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count    Dtype \n",
      "---  ------             --------------    ----- \n",
      " 0   merchant           1296675 non-null  object\n",
      " 1   category           1296675 non-null  object\n",
      " 2   first              1296675 non-null  object\n",
      " 3   last               1296675 non-null  object\n",
      " 4   gender             1296675 non-null  object\n",
      " 5   street             1296675 non-null  object\n",
      " 6   city               1296675 non-null  object\n",
      " 7   state              1296675 non-null  object\n",
      " 8   job                1296675 non-null  object\n",
      " 9   dob                1296675 non-null  object\n",
      " 10  trans_num          1296675 non-null  object\n",
      " 11  trans_month        1296675 non-null  object\n",
      " 12  trans_day_of_week  1296675 non-null  object\n",
      " 13  trans_date         1296675 non-null  object\n",
      "dtypes: object(14)\n",
      "memory usage: 148.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# all object columns\n",
    "X.select_dtypes(include=['object']).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f735a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 12 columns):\n",
      " #   Column               Non-Null Count    Dtype  \n",
      "---  ------               --------------    -----  \n",
      " 0   cc_num               1296675 non-null  int64  \n",
      " 1   amt                  1296675 non-null  float64\n",
      " 2   zip                  1296675 non-null  int64  \n",
      " 3   lat                  1296675 non-null  float64\n",
      " 4   long                 1296675 non-null  float64\n",
      " 5   city_pop             1296675 non-null  int64  \n",
      " 6   unix_time            1296675 non-null  int64  \n",
      " 7   merch_lat            1296675 non-null  float64\n",
      " 8   merch_long           1296675 non-null  float64\n",
      " 9   trans_hour_of_day    1296675 non-null  int32  \n",
      " 10  trans_count_per_day  1296675 non-null  int64  \n",
      " 11  birth_year           1296675 non-null  int32  \n",
      "dtypes: float64(5), int32(2), int64(5)\n",
      "memory usage: 118.7 MB\n"
     ]
    }
   ],
   "source": [
    "# all numerical columns\n",
    "X.select_dtypes(include=['float64', 'int64', 'int32']).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ce473b",
   "metadata": {},
   "source": [
    "Looking at all the object Dtype columns, I am most interested in keeping the gender, state, job, trans_month, and trans_day_of_week. \n",
    "\n",
    "I am opting to remove the street, zip, and city columns, as I think training on the state will be enough. If, after we've trained a model and looked at the feature coefficients, the state in which a person lives carries a high probablity of a fraudulent transaction, we can train a new model to take into consideration the mroe in depth locational features. \n",
    "\n",
    "Similarly, I am omitting the cc_number, first, last, dob, lat, long, trans_num, merch_lat, merch_long, and trans_date_trans_time. Removing these columns will reduces noise in our data which will increase the accuracy and reliability of our model.\n",
    "\n",
    "The following columns are dropped from our dataset below:\n",
    "- trans_date_trans_time\n",
    "- cc_num  \n",
    "- first  \n",
    "- last  \n",
    "- street  \n",
    "- city \n",
    "- zip  \n",
    "- lat\n",
    "- long\n",
    "- dob\n",
    "- trans_num  \n",
    "- unix_time\n",
    "- merch_lat\n",
    "- merch_long\n",
    "- trans_date\n",
    "\n",
    "and we will be keeping the following columns:\n",
    "- merchant\n",
    "- category\n",
    "- amt\n",
    "- gender\n",
    "- state\n",
    "- city_pop\n",
    "- job\n",
    "- trans_month\n",
    "- trans_day_of_week\n",
    "- trans_hour_of_day\n",
    "- trans_count_per_day\n",
    "- birth_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47eff5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 12 columns):\n",
      " #   Column               Non-Null Count    Dtype  \n",
      "---  ------               --------------    -----  \n",
      " 0   merchant             1296675 non-null  object \n",
      " 1   category             1296675 non-null  object \n",
      " 2   amt                  1296675 non-null  float64\n",
      " 3   gender               1296675 non-null  object \n",
      " 4   state                1296675 non-null  object \n",
      " 5   city_pop             1296675 non-null  int64  \n",
      " 6   job                  1296675 non-null  object \n",
      " 7   trans_month          1296675 non-null  object \n",
      " 8   trans_day_of_week    1296675 non-null  object \n",
      " 9   trans_hour_of_day    1296675 non-null  int32  \n",
      " 10  trans_count_per_day  1296675 non-null  int64  \n",
      " 11  birth_year           1296675 non-null  int32  \n",
      "dtypes: float64(1), int32(2), int64(2), object(7)\n",
      "memory usage: 118.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# dropping all columns we are not interested in\n",
    "X = X.drop(columns=['cc_num',\n",
    "                    'first',\n",
    "                    'last',\n",
    "                    'street',\n",
    "                    'city',\n",
    "                    'zip',\n",
    "                    'dob',\n",
    "                    'lat',\n",
    "                    'long',\n",
    "                    'trans_num',\n",
    "                    'unix_time',\n",
    "                    'merch_lat',\n",
    "                    'merch_long',\n",
    "                    'trans_date_trans_time',\n",
    "                    'trans_date'\n",
    "                    ])\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37472564",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Correlation Analysis\n",
    "\n",
    "Correlation analysis can help us determine how dependent two numerical variables are on each other. The larger the correlation value (either negative or positive), the more related the two variable are. We won't necessarily remove one of the variable it's correlation value with another variable is large, but we'll look at the relationship and determine if keeping both would introduce reduntant, noisy data or not.\n",
    "\n",
    "Since we would like to calculate the correlation of our numerical *and* categorical data, we need to convert categorical features into numerical features without heirarchy. To do this, I will use sci-kit learn's [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "num_cols = X.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "# I am opting to set sparse_output to False to get a dense array back since this is a small dataset and takes about 20 seconds to run on my machine. \n",
    "# If this takes too long on a different machine, consider setting sparse_output to True for a faster run time.\n",
    "ohe = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "X_ohe = ohe.fit_transform(X.drop(columns=cat_cols))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
