{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6c25d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "59e9055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our training data and take the first column as the index column\n",
    "fraud_train = pd.read_csv('fraudTrain.csv', index_col=0)\n",
    "\n",
    "# split the training data into x and y\n",
    "X = fraud_train.drop(columns='is_fraud')\n",
    "y = fraud_train['is_fraud']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "652b9429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   trans_date_trans_time  1296675 non-null  object \n",
      " 1   cc_num                 1296675 non-null  int64  \n",
      " 2   merchant               1296675 non-null  object \n",
      " 3   category               1296675 non-null  object \n",
      " 4   amt                    1296675 non-null  float64\n",
      " 5   first                  1296675 non-null  object \n",
      " 6   last                   1296675 non-null  object \n",
      " 7   gender                 1296675 non-null  object \n",
      " 8   street                 1296675 non-null  object \n",
      " 9   city                   1296675 non-null  object \n",
      " 10  state                  1296675 non-null  object \n",
      " 11  zip                    1296675 non-null  int64  \n",
      " 12  lat                    1296675 non-null  float64\n",
      " 13  long                   1296675 non-null  float64\n",
      " 14  city_pop               1296675 non-null  int64  \n",
      " 15  job                    1296675 non-null  object \n",
      " 16  dob                    1296675 non-null  object \n",
      " 17  trans_num              1296675 non-null  object \n",
      " 18  unix_time              1296675 non-null  int64  \n",
      " 19  merch_lat              1296675 non-null  float64\n",
      " 20  merch_long             1296675 non-null  float64\n",
      "dtypes: float64(5), int64(4), object(12)\n",
      "memory usage: 217.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94102</th>\n",
       "      <td>2019-02-25 08:24:40</td>\n",
       "      <td>374497717543058</td>\n",
       "      <td>fraud_Funk Group</td>\n",
       "      <td>grocery_net</td>\n",
       "      <td>20.00</td>\n",
       "      <td>Linda</td>\n",
       "      <td>Hurst</td>\n",
       "      <td>F</td>\n",
       "      <td>31701 Tucker Square Suite 893</td>\n",
       "      <td>Wilton</td>\n",
       "      <td>ND</td>\n",
       "      <td>58579</td>\n",
       "      <td>47.1709</td>\n",
       "      <td>-100.7944</td>\n",
       "      <td>1190</td>\n",
       "      <td>Designer, ceramics/pottery</td>\n",
       "      <td>1948-06-30</td>\n",
       "      <td>1595dec12f6f19ceaae9b7df0f8af5c0</td>\n",
       "      <td>1330158280</td>\n",
       "      <td>46.398331</td>\n",
       "      <td>-99.813959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198791</th>\n",
       "      <td>2019-04-12 19:50:15</td>\n",
       "      <td>4428154703770710</td>\n",
       "      <td>fraud_Prosacco, Kreiger and Kovacek</td>\n",
       "      <td>home</td>\n",
       "      <td>284.88</td>\n",
       "      <td>Brittany</td>\n",
       "      <td>Guerra</td>\n",
       "      <td>F</td>\n",
       "      <td>79209 Gary Dale</td>\n",
       "      <td>Colton</td>\n",
       "      <td>WA</td>\n",
       "      <td>99113</td>\n",
       "      <td>46.5901</td>\n",
       "      <td>-117.1692</td>\n",
       "      <td>761</td>\n",
       "      <td>Chief Marketing Officer</td>\n",
       "      <td>1943-06-30</td>\n",
       "      <td>0ed26b649ed0fce94d8e632b7208dea0</td>\n",
       "      <td>1334260215</td>\n",
       "      <td>45.687331</td>\n",
       "      <td>-117.488135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238587</th>\n",
       "      <td>2020-05-31 21:50:53</td>\n",
       "      <td>213148039875802</td>\n",
       "      <td>fraud_Langworth, Boehm and Gulgowski</td>\n",
       "      <td>shopping_net</td>\n",
       "      <td>5.07</td>\n",
       "      <td>Jill</td>\n",
       "      <td>Jacobs</td>\n",
       "      <td>F</td>\n",
       "      <td>034 Kimberly Mountains</td>\n",
       "      <td>Brandon</td>\n",
       "      <td>FL</td>\n",
       "      <td>33510</td>\n",
       "      <td>27.9551</td>\n",
       "      <td>-82.2966</td>\n",
       "      <td>79613</td>\n",
       "      <td>Environmental consultant</td>\n",
       "      <td>1978-11-30</td>\n",
       "      <td>7096316ec1a4b261e8613013827abae7</td>\n",
       "      <td>1370037053</td>\n",
       "      <td>27.254081</td>\n",
       "      <td>-81.974799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        trans_date_trans_time            cc_num  ...  merch_lat  merch_long\n",
       "94102     2019-02-25 08:24:40   374497717543058  ...  46.398331  -99.813959\n",
       "198791    2019-04-12 19:50:15  4428154703770710  ...  45.687331 -117.488135\n",
       "1238587   2020-05-31 21:50:53   213148039875802  ...  27.254081  -81.974799\n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first look at our data\n",
    "X.info()\n",
    "X.sample(3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f775f22",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## 1. Finding and Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b01dbe95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what columns have null values?\n",
    "sum(X.isnull().sum()) + y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5586ff34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what columns have missing/na values?\n",
    "sum(X.isna().sum()) + y.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42263e5e",
   "metadata": {},
   "source": [
    "No handling of missing values is required as there are no missing values in our trainng dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33756333",
   "metadata": {},
   "source": [
    "## 2. Finding and Removing Outliers\n",
    "\n",
    "Removing outliers is an important part of preprocessing as it can:\n",
    "- distort data analysis \n",
    "- reduce machine learning model accuracy and generalization\n",
    "- impact visual data, skewing the scale \n",
    "- more?\n",
    "\n",
    "For numeric columns (those with an integer or float data type), I chose to use the [Interquartile Range Method](https://online.stat.psu.edu/stat200/lesson/3/3.2) of finding and removing outliers. \n",
    "\n",
    "Categorical, or \n",
    "\n",
    "## FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "16038acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the numerical columns\n",
    "num_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "num_cols\n",
    "\n",
    "# calculate the IQR lower and upper bounds for each numerical column\n",
    "def iqr_bounds(col):\n",
    "    Q1 = col.quantile(0.25)\n",
    "    Q3 = col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bb56025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_num: lower=-6513275846701038.0, upper=1.133557426847813e+16\n",
      "amt: lower=-100.58499999999998, upper=193.375\n",
      "zip: lower=-42470.5, upper=140749.5\n",
      "lat: lower=23.640650000000004, upper=52.920249999999996\n",
      "long: lower=-121.75800000000001, upper=-55.198\n",
      "city_pop: lower=-28634.5, upper=49705.5\n",
      "unix_time: lower=1307798793.0, upper=1390337325.0\n",
      "merch_lat: lower=23.898184000000008, upper=52.79255199999999\n",
      "merch_long: lower=-121.88799400000002, upper=-55.24607799999998\n"
     ]
    }
   ],
   "source": [
    "# lets look at the lower and upper bounds for each numerical column\n",
    "for col in num_cols:\n",
    "    lower, upper = iqr_bounds(X[col])\n",
    "    print(f\"{col}: lower={lower}, upper={upper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5a261fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_num: 118789 outliers\n",
      "amt: 67290 outliers\n",
      "zip: 0 outliers\n",
      "lat: 4679 outliers\n",
      "long: 49922 outliers\n",
      "city_pop: 242674 outliers\n",
      "unix_time: 0 outliers\n",
      "merch_lat: 4967 outliers\n",
      "merch_long: 41994 outliers\n"
     ]
    }
   ],
   "source": [
    "# how many outliers are there in each numerical column?\n",
    "for col in num_cols:\n",
    "    lower, upper = iqr_bounds(X[col])\n",
    "    outliers = fraud_train[(fraud_train[col] < lower) | (fraud_train[col] > upper)]\n",
    "    print(f\"{col}: {len(outliers)} outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddcae0",
   "metadata": {},
   "source": [
    "I won't be dropping the rows with outlier values before training a model on the dataset, as I'm interested in comparing how the model performs with and without the outlier values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6447216",
   "metadata": {},
   "source": [
    "### 3. Feature Selection & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d0dbf",
   "metadata": {},
   "source": [
    "Before training a model, I want to get a good idea of our data. I ask myself questions such as *\"is this information redundnat?\"* or *\"can this data generalized or are all values unique?\"*. If the answer to either question is yes, it is usually best to remove the column. However, sometimes there is information we can extract from the column that may not be redundant and can be generalized or categorized. \n",
    "\n",
    "\n",
    "One example in our data is the trans_time_trans_date column. At the moment, this column may not help us detect fraud very well since almost every entry is unqiue (as shown below), but there is some interesting information we may want to look at within the column. Information such as day of the week or the hour in which the transaction took place could help us identify fraudulent transactions more accurately. Based on our columns printed below, I plan to extract the following information:\n",
    "\n",
    "## FIX ME:\n",
    "- cc_num: number of transaction in past **X time** (days? weeks? months?)\n",
    "- trans_date_trans_time: month, day, and hour of transaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "755eb76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.31% of the values in trans_date_trans_time are unique.\n",
      "0.08% of the values in cc_num are unique.\n",
      "0.05% of the values in merchant are unique.\n",
      "0.0% of the values in category are unique.\n",
      "4.08% of the values in amt are unique.\n",
      "0.03% of the values in first are unique.\n",
      "0.04% of the values in last are unique.\n",
      "0.0% of the values in gender are unique.\n",
      "0.08% of the values in street are unique.\n",
      "0.07% of the values in city are unique.\n",
      "0.0% of the values in state are unique.\n",
      "0.07% of the values in zip are unique.\n",
      "0.07% of the values in lat are unique.\n",
      "0.07% of the values in long are unique.\n",
      "0.07% of the values in city_pop are unique.\n",
      "0.04% of the values in job are unique.\n",
      "0.07% of the values in dob are unique.\n",
      "100.0% of the values in trans_num are unique.\n",
      "98.31% of the values in unix_time are unique.\n",
      "96.23% of the values in merch_lat are unique.\n",
      "98.39% of the values in merch_long are unique.\n"
     ]
    }
   ],
   "source": [
    "def unique_percentage(column):\n",
    "    return (len(X[column].unique())) / len(X[column]) * 100\n",
    "\n",
    "for column in X.columns:\n",
    "    print(f'{round(unique_percentage(column),2)}% of the values in {column} are unique.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "49f2ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   trans_date_trans_time  1296675 non-null  object \n",
      " 1   cc_num                 1296675 non-null  int64  \n",
      " 2   merchant               1296675 non-null  object \n",
      " 3   category               1296675 non-null  object \n",
      " 4   amt                    1296675 non-null  float64\n",
      " 5   first                  1296675 non-null  object \n",
      " 6   last                   1296675 non-null  object \n",
      " 7   gender                 1296675 non-null  object \n",
      " 8   street                 1296675 non-null  object \n",
      " 9   city                   1296675 non-null  object \n",
      " 10  state                  1296675 non-null  object \n",
      " 11  zip                    1296675 non-null  int64  \n",
      " 12  lat                    1296675 non-null  float64\n",
      " 13  long                   1296675 non-null  float64\n",
      " 14  city_pop               1296675 non-null  int64  \n",
      " 15  job                    1296675 non-null  object \n",
      " 16  dob                    1296675 non-null  object \n",
      " 17  trans_num              1296675 non-null  object \n",
      " 18  unix_time              1296675 non-null  int64  \n",
      " 19  merch_lat              1296675 non-null  float64\n",
      " 20  merch_long             1296675 non-null  float64\n",
      "dtypes: float64(5), int64(4), object(12)\n",
      "memory usage: 217.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# all columns and data types\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4adb6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: below but unix time? fraud might not be happening in the local timezone of the transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "89797b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: DUPLICATE CC numbers BEFORE DROPPING them\n",
    "# count trans in the last 24 hours? week? month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "de5f3d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_month</th>\n",
       "      <th>trans_day_of_week</th>\n",
       "      <th>trans_hour_of_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>January</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296670</th>\n",
       "      <td>June</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296671</th>\n",
       "      <td>June</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296672</th>\n",
       "      <td>June</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296673</th>\n",
       "      <td>June</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296674</th>\n",
       "      <td>June</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1296675 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        trans_month trans_day_of_week  trans_hour_of_day\n",
       "0           January           Tuesday                  0\n",
       "1           January           Tuesday                  0\n",
       "2           January           Tuesday                  0\n",
       "3           January           Tuesday                  0\n",
       "4           January           Tuesday                  0\n",
       "...             ...               ...                ...\n",
       "1296670        June            Sunday                 12\n",
       "1296671        June            Sunday                 12\n",
       "1296672        June            Sunday                 12\n",
       "1296673        June            Sunday                 12\n",
       "1296674        June            Sunday                 12\n",
       "\n",
       "[1296675 rows x 3 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming 'trans_date_trans_time' into day of week, hour of day, and month\n",
    "X['trans_date_trans_time'] = pd.to_datetime(X['trans_date_trans_time'])\n",
    "X['trans_month'] = X['trans_date_trans_time'].dt.month_name()\n",
    "X['trans_day_of_week'] = X['trans_date_trans_time'].dt.day_name()\n",
    "X['trans_hour_of_day'] = X['trans_date_trans_time'].dt.hour\n",
    "\n",
    "\n",
    "X[['trans_month', 'trans_day_of_week', 'trans_hour_of_day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cb92a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ce473b",
   "metadata": {},
   "source": [
    "Looking at all the object Dtype columns, I am most interested in keeping the gender, state, job, dob, trans_month, and trans_day_of_week. \n",
    "\n",
    "I have chosen to omit the following columns:\n",
    "- cc_num  \n",
    "- first  \n",
    "- last  \n",
    "- street  \n",
    "- city  \n",
    "- zip  \n",
    "- lat\n",
    "- long\n",
    "- merch_lat\n",
    "- merch_long\n",
    "- trans_num  \n",
    "- trans_date_trans_time \n",
    "\n",
    "I opted to remove the street, zip, and city columns, as I think training on the state will be enough. Removing these columns aslo reduces noise in our data which will increase the accuracy and reliability of our model. If, after we've trained a model and looked at the feature coefficients, the state in which a person lives carries a high probablity of a fraudulent transaction, we can train a new model to take into consideration the mroe in depth locational features. \n",
    "\n",
    "Similarly, I am omitting the cc_number, first, last, lat, long, trans_num, merch_lat, merch_long, and trans_date_trans_time as all of these columns introduce noise in our dataset and can reduce the reliability and accuracy of a model trained on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "47eff5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dropping all columns we are not interested in\n",
    "# X = X.drop(columns=['cc_num',\n",
    "#                     'first',\n",
    "#                     'last',\n",
    "#                     'street',\n",
    "#                     'city',\n",
    "#                     'zip',\n",
    "#                     'lat',\n",
    "#                     'long',\n",
    "#                     'trans_num',\n",
    "#                     'unix_time',\n",
    "#                     'merch_lat',\n",
    "#                     'merch_long',\n",
    "#                     'trans_date_trans_time'\n",
    "#                     ])\n",
    "\n",
    "# X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f133ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.preprocessing import OneHotEncoder\n",
    "# ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "# X_ohe = ohe.fit_transform(X.select_dtypes(include=['object']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37472564",
   "metadata": {},
   "source": [
    "### 4. Correlation Analysis\n",
    "\n",
    "Before we analyze the correlation between all numerical columns, we need to convert categorical features into numerical features without heirarchy. To do this, I will use sci-kit learn's [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
